<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Gary Ip - monkaS</title>
<link rel="shortcut icon" href="https://github.com/gippoo/gippoo.github.io/raw/master/favicon.ico" type="image/x-icon">
<link rel="icon" href="https://github.com/gippoo/gippoo.github.io/raw/master/favicon.ico" type="image/x-icon">

<style>
    body {
        font-family: "Trebuchet MS";
    }
    
    a.header:link {
        color: #bfd8ff;
    }
    a.header:visited {
        color: #bfd8ff;
    }  
    a.header:hover {
        color: #6da6ff;
    }
    
    div.header {
        position: absolute;
        left: 0;
        top: 0;
        right: 0;
        height: 54px;
        background-color: #4a70ad;
    }
    
    div.title_text {
        position: absolute;
        left: 10px;
        top: 10px;
        font-size: 24px;
	    color: white;
    }
    
    pre.code {
        font-size: 14px;
        background-color: #dee1e3;
        width: 600px;
    }
</style>

</head>


<body>


<div class="header">
    <div class="title_text">
    	<a class="header" href="https://gippoo.github.io/" style="text-decoration:none;">Gary Ip</a> | 
        <a class="header" href="https://gippoo.github.io/resume/" style="text-decoration:none;">Resume</a> | 
        <a class="header" href="https://github.com/gippoo" style="text-decoration:none;">GitHub</a> | 
        <a class="header" href="https://www.linkedin.com/in/gary-ip27/" style="text-decoration:none;">LinkedIn</a>
    </div>
</div>


<div style="position: relative; top: 54px; text-align: center; font-family: Times">
    <figure>
        <img src="https://vignette.wikia.nocookie.net/aceattorney/images/0/02/Maya_Fey_Portrait_AA6.png/revision/latest/scale-to-width-down/310?cb=20160407182727" width="155" height="232"/>
        <figcaption style="font-size: 12px;">Source: https://aceattorney.fandom.com/wiki/Maya_Fey</figcaption>
    </figure>
    <span style="font-size:32px;">Building a Chatbot</span>
</div>
<div style="position: relative; top: 54px; margin:auto; width: 700px; font-size:20px; font-family: Times;">
    <p>After watching I'm not a Robot, I was inspired to create a chatbot using deep learning. After doing some research, it seems that many examples of conversational chatbots are trained on movie conversations or collections of comments from online forums such as Reddit. <br>To put my own spin on it, I decided to train my chatbot the personality of a character from a video game. Turns out that the Phoenix Wright series is great for this as the games are extremely rich in dialouge exchanged between characters. I settled on using Maya (pictured above) to be the character the chatbot would learn from.
    </p>
    <p>I found a <a href="https://gamefaqs.gamespot.com/ds/925589-phoenix-wright-ace-attorney/faqs/42767">game script</a> from the first game in the series. Here is a small snippet from the script:
        <pre style="font-size:16px">
            -Maya
            Ph-Phoenix...?
            
            -Phoenix
            Well... court will be opening
            for session soon.
            
            -Maya
            What? But wait!
            
            -Maya
            Your defense attorney isn't
            even here yet! He's not...
            
            -Phoenix
            I'll be defending myself.
            
            -Maya
            Whaaaat!?
            
            -Phoenix
            Okay, let's do this.
        </pre>
    </p>
    <p>Unfortunately, they entire document was not always this clean. There would be a lot of random extra spaces or symbols used. For example some parts of the script looked like this:
        <pre style="font-size:16px">
            * -Maya
            * Maybe it should just be
            * a big "L" for "Lawyer"?
            *
            * -Phoenix
            * Hmm... I'm not so
            * sure about that.
            *
            ********************************************
            
            ***Present other****************************
            *
            * -Phoenix
            * Um, Detective.
        </pre>
    </p>
    <p>I did some basic cleaning first using the built in 'Find and Replace' functionality of notepad and then using Python string manipulations, such as making all text lowercase.
        <br>Then I wrote an algorithm to extract all of Maya's lines, which are the 'replies', and also the lines before each of Maya's lines, which are the 'prompts'.
    </p>
    <p>In the end we are left with two arrays of strings: one with prompts and one with Maya's responses. Both arrays are of length 2116. Let's look at the first 5 of these pairs:
        <pre class="code"><code>
    for i in range(5):
        print("PROMPT: "+prompt[i])
        print("REPLY: "+reply[i])
        print('\n')
        </code></pre>
    </p>
    <p>
        <pre style="font-size:16px">
        PROMPT: it's okay. i work here.
        REPLY: maya...
        
        
        PROMPT: maya...
        REPLY: maya fey.
        
        
        PROMPT: ...
        REPLY: i came in... the room was dark.
        
        
        PROMPT: i came in... the room was dark.
        REPLY: and sis... sis...!
        
        
        PROMPT: so, you're the chief's...?
        REPLY: sister. i'm her younger sister.
        </pre>
    </p>
    
    <p>Some improvements could be made here. For example, Maya sometimes says multiple lines in succession so when extracting prompt and reply pairs, there are cases where she sometimes responds to herself. You can see this in the output above where she replies to her own reply. <br>Perhaps I'll come back and refine this at a later time, but it should still be okay since for these intial stages we mainly care about gathering data on Maya's replies to ANYTHING.</p>
    
    <p>Next, we need to decide whether we want to preprocess the text at a character level or at the word level. There are advantages and disadvatanges to both but I decided to go with the character level because of faster training time.</p>
    <p>We will be using an LSTM encoder-decoder model outlined in the <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html">Keras Blog</a> post on seq2seq models.
    </p>
    <p>This requires us to have 3 arrays of data to use for training: the input prompt, the input reply, and the output reply. <br>-The input prompt is simply an array of the prompts. <br>-The input reply is each reply with a START token placed at the beginning. <br>-The output replies have no START token but instead have an END token placed at the end of each line. The result is two arrays of replies with the output replies offset by 1 position.</p>
    <p>To preprocess the text, we build a dictionary that maps each character to a unique index value. We actually need two of these dictionaries, one for the prompts, and one for the replies since there may be characters present in the prompts which are not in the replies and vice versa. While we're at it, we can also create dictionaries that map the index value back to the character.
        <pre class="code"><code>
    prompt_chars = set(' '.join(prompt))
    reply_chars = set(' '.join(reply))
    
    prompt_char_to_ix = dict((c, i) for i, c in enumerate(prompt_chars))
    prompt_ix_to_char = dict((i, c) for i, c in enumerate(prompt_chars))
    reply_char_to_ix = dict((c, i) for i, c in enumerate(reply_chars))
    reply_ix_to_char = dict((i, c) for i, c in enumerate(reply_chars))
        </code></pre>
    </p>
    <p>Now, for each line of dialouge, we convert each character into its index value and then one-hot encode each value. The result of each line is a matrix of shape:<br> (max line length, number of unique characters) 
    <pre class="code"><code>
    # Processing the prompts
    X = []
    
    for line in prompt:
        one_hot = np.zeros((max_prompt_len, prompt_feature_size))
        x_seq = [prompt_char_to_ix[c] for c in line]
        for j in range(len(x_seq)):
            one_hot[j][x_seq[j]] = 1
        X.append(one_hot)
    X = np.asarray(X)
    
    # Processing the replies
    # Function to add start and end tokens
    def start_end(line):
        line = '\t'+line+'\n'
        return line
        
    Y = [start_end(line) for line in reply]
    
    y = []

    for line in Y:
        one_hot = np.zeros((max_reply_len, reply_feature_size))
        y_seq = [reply_char_to_ix[c] for c in line]
        for j in range(len(y_seq)):
            one_hot[j][y_seq[j]] = 1
        y.append(one_hot)
    y = np.asarray(y)
    
    # Split replies into input and output replies
    y_input = y[:,:max_reply_len-1,:]
    y_output = y[:,1:,:]
    </code></pre>

    </p>
    <p>As an example, we can look at one prompt and reply before and after processing.
    </p>
    <p>Finally we can build the model and start training. The model is largely identical to the one in the Keras Blog.
    <pre class="code"><code>
    encoder_inputs = Input(shape=(None, prompt_feature_size))
    encoder = LSTM(128, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)
    encoder_states = [state_h, state_c]

    decoder_inputs = Input(shape=(None, reply_feature_size))
    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,
                                         initial_state=encoder_states)
    decoder_dense = Dense(reply_feature_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)
    
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    
    model.summary()        
        
    </code></pre>
    <pre style="font-size:12px">
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, None, 52)     0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, None, 51)     0                                            
__________________________________________________________________________________________________
lstm_3 (LSTM)                   [(None, 128), (None, 92672       input_3[0][0]                    
__________________________________________________________________________________________________
lstm_4 (LSTM)                   [(None, None, 128),  92160       input_4[0][0]                    
                                                                 lstm_3[0][1]                     
                                                                 lstm_3[0][2]                     
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, None, 51)     6579        lstm_4[0][0]                     
==================================================================================================
Total params: 191,411
Trainable params: 191,411
Non-trainable params: 0
    </pre></p>
    <p>I trained the model for 700 epochs which probably was too much in hindsight. Half the amount was sufficient. Also note that we didn't need to hold out any data for validation since there isn't really such a thing as a "correct" reply that we are trying to get.
        <pre class="code"><code>
    model.fit([X, y_input], y_output, epochs=700)
        </code></pre>
    </p>
    <p>Before we can use the trained model, we need some helper functions to process any prompts we give it and also to change the model output back into text.
    <pre class="code"><code>
    # Function to process our own prompts
    def process_question(question):
        question_vec = [prompt_char_to_ix[c] for c in question]
        
        one_hot = np.zeros((max_prompt_len, prompt_feature_size))
        
        for j in range(len(question_vec)):
            one_hot[j][question_vec[j]] = 1
        
        return one_hot.reshape(-1,max_prompt_len,prompt_feature_size)
        
    # Function to get a reply from the model and convert it to text
    def gen_reply(question):
        question = process_question(question)
        
        states_value = encoder_model.predict(question)
        
        target_seq = np.zeros((1, 1, reply_feature_size))
        target_seq[0, 0, reply_char_to_ix['\t']] = 1.
        
        stop_condition = False
        decoded_sentence = ''
        
        while not stop_condition:
            output_tokens, h, c = decoder_model.predict([target_seq] 
                                                        + states_value)
            
            sampled_token_index = np.argmax(output_tokens[0, -1, :])
            sampled_char = reply_ix_to_char[sampled_token_index]
            decoded_sentence += sampled_char
            
            if (sampled_char == '\n' 
                or len(decoded_sentence) > max_reply_len):
                stop_condition = True
    
            target_seq = np.zeros((1, 1, reply_feature_size))
            target_seq[0, 0, sampled_token_index] = 1.
            
            states_value = [h, c]
            
        return decoded_sentence
    </code></pre></p>
    <p>FINALLY we can start interacting with our chatbot.
    <pre style="font-size:16px">
    Me: hi
    MayaBot: i was really sorry'se callma!
    
    Me: what are you sorry about?
    MayaBot: this is the ficoston! too...
    
    Me: what is the ficoston?
    MayaBot: this is all tied to the dl-6 incident...
    
    Me: what happened in the incident?
    MayaBot: old wird mos like the sild, i'm noting on locken "sus "lissel, ergs!
    
    Me: uh that didn't make sense
    MayaBot: h-huh!? o-w-ow...
    
    Me: that actually made sense
    MayaBot: nick!
    
    Me: im not nick
    MayaBot: what!?
    
    Me: ok bye.
    MayaBot: nick! look! a lott!
    </pre>

    </p>
    
    
</div>
<div style="height: 100px"></div>
</body>
</html>
