<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Gary Ip - Chatbot</title>
<link rel="shortcut icon" href="https://github.com/gippoo/gippoo.github.io/raw/master/favicon.ico" type="image/x-icon">
<link rel="icon" href="https://github.com/gippoo/gippoo.github.io/raw/master/favicon.ico" type="image/x-icon">

<style>
    body {
        font-family: "Trebuchet MS";
    }
    
    a.header:link {
        color: #bfd8ff;
    }
    a.header:visited {
        color: #bfd8ff;
    }  
    a.header:hover {
        color: #6da6ff;
    }
    
    a:link {
        color: #527ba1;
    }
    a:visited {
        color: #527ba1;
    }  
    a:hover {
        color: #bcd1e6;
    }
    
    div.header {
        position: absolute;
        left: 0;
        top: 0;
        right: 0;
        height: 54px;
        background-color: #4a70ad;
    }
    
    div.title_text {
        position: absolute;
        left: 10px;
        top: 10px;
        font-size: 24px;
	    color: white;
    }
    
    pre.code {
        font-size: 14px;
        background-color: #dee1e3;
        width: 600px;
    }
</style>

</head>


<body>


<div class="header">
    <div class="title_text">
    	<a class="header" href="https://gippoo.github.io/" style="text-decoration:none;">Gary Ip</a> | 
        <a class="header" href="https://gippoo.github.io/resume/" style="text-decoration:none;">Resume</a> | 
        <a class="header" href="https://github.com/gippoo" style="text-decoration:none;">GitHub</a> | 
        <a class="header" href="https://www.linkedin.com/in/gary-ip27/" style="text-decoration:none;">LinkedIn</a>
    </div>
</div>


<div style="position: relative; top: 54px; text-align: center; font-family: Times">
    <figure>
        <img src="https://vignette.wikia.nocookie.net/aceattorney/images/0/02/Maya_Fey_Portrait_AA6.png/revision/latest/scale-to-width-down/310?cb=20160407182727" width="155" height="232"/>
        <figcaption style="font-size: 12px;">Source: https://aceattorney.fandom.com/wiki/Maya_Fey</figcaption>
    </figure>
    <span style="font-size:34px;">Building a Chatbot</span>
</div>
<div style="position: relative; top: 54px; margin:auto; width: 700px; font-size:18px; font-family: Times;">
    <p><i>
        If you just want to see a demo, click <a href="https://gippoo.github.io/chatbot/demo/">here</a>.
    </i></p>
    <span style="font-size:28px; font-family:arial;"><u>Motivation</u></span>
    <p>After watching I'm not a Robot and obsessing over it, I was inspired to create some kind of AI you could interact with in a human-like way. Making a chatbot seemed like a good place to start.</p>
    <p>There are a multitude of chatbots made for different purposes and are coded in different ways.
        <br>Some chatbots are made to help people with specific tasks such as getting information. I decided to make a conversational chatbot whose purpose is just to have random conversations. <br>While some chatbots are rule-based with several hard coded responses, I will be training mines using deep learning and natural language processing.</p>
    <span style="font-size:28px; font-family:arial;"><u>Data</u></span>
    <p>The replies that this chatbot will give is going to largely depend on the data it is trained on.</p>
    <p>I needed to find a dataset containing pairs of prompts and replies. Through some Google searches, I discovered that most examples of conversational chatbots are trained on the <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie Corpus</a> or a dataset of <a href="https://www.kaggle.com/reddit/reddit-comments-may-2015">Reddit comments</a>. These datasets contain several prompts and respones to those prompts.</p>
    <p>However, I didn't want to do something that was already done. So to put my own spin on it, I decided to train my chatbot to respond like a character from a video game. 
    <br>I thought to myself: It would be so cool if you could talk to your favorite video game character and get replies from them.</p>
    <p>What character should I use? 
    <br>After some thought, I figured that the Phoenix Wright series would be great for this as the games are rich in text and have many exchanges between characters. I settled on using Maya (pictured above) to be the character the chatbot would learn from.</p>
    </p>
    <p>Fortunately, the games have been transcribed by fans so all the data I needed was already out there. I used a <a href="https://gamefaqs.gamespot.com/ds/925589-phoenix-wright-ace-attorney/faqs/42767">game script</a> from the first game in the series.
    <br>Here is a small snippet from the script:
        <pre style="font-size:16px">
            -Maya
            Ph-Phoenix...?
            
            -Phoenix
            Well... court will be opening
            for session soon.
            
            -Maya
            What? But wait!
            
            -Maya
            Your defense attorney isn't
            even here yet! He's not...
            
            -Phoenix
            I'll be defending myself.
            
            -Maya
            Whaaaat!?
            
            -Phoenix
            Okay, let's do this.
        </pre>
    </p>
    <p>Unfortunately, the entire document was not always this clean. There would be a lot of random extra spaces or symbols used. For example some parts of the script looked like this:
        <pre style="font-size:16px">
            * -Maya
            * Maybe it should just be
            * a big "L" for "Lawyer"?
            *
            * -Phoenix
            * Hmm... I'm not so
            * sure about that.
            *
            ********************************************
            
            ***Present other****************************
            *
            * -Phoenix
            * Um, Detective.
        </pre>
    </p>
    <p>All those asterisks were quite bothersome. 
    <br>I did some basic cleaning first using the built in 'Find and Replace' functionality of notepad and then more nuanced cleaning by writing a few Python scripts.</p>
        <p>Then I wrote a function to extract the lines of any character as well as the line directly before it. Of course, I used this function to get Maya's lines which were stored as the 'replies'. The previous lines directly before each of Maya's lines, were stored as the 'prompts'.
    </p>
    <p>In the end we are left with 2116 prompt-reply pairs. This is a very small collection of data considering the amount of text that typically gets used in NLP projects. <br>Anyways, let's look at the first 5 of these prompt-reply pairs:

    </p>
    <p>
        <pre style="font-size:16px">
        PROMPT: it's okay. i work here.
        REPLY: maya...
        
        
        PROMPT: maya...
        REPLY: maya fey.
        
        
        PROMPT: ...
        REPLY: i came in... the room was dark.
        
        
        PROMPT: i came in... the room was dark.
        REPLY: and sis... sis...!
        
        
        PROMPT: so, you're the chief's...?
        REPLY: sister. i'm her younger sister.
        </pre>
    </p>
    
    <p>Some improvements could be made here. In the original script, Maya sometimes says multiple lines in succession so, when extracting prompt and reply pairs, there are cases where she sometimes responds to herself. You can see this in the output above where she replies to her own reply.</p>
    <p>There are probably some regex expressions that could be used to address this but I couldn't think of anything simple to implement. So I decided to proceed with what I had which should still be okay since we mainly care about gathering data on Maya's replies to ANYTHING (even herself).</p>
    
    <span style="font-size:28px; font-family:arial;"><u>Preprocessing Data</u></span>
    <p>Most, if not all, statistical models cannot work with pure text. They only work with numbers. So I needed to convert text into numbers to feed into the model. <br>The first step of this is to tokenize the data. The two most common ways of doing this is breaking down each line into every individual character. Or breaking down each line into every individual word.</p>
    <p>I wanted the chatbot to be able to respond to any input from a human. If I had tokenized the prompts at the word level, then because of the small data size, there would be a lot of words the user could type which would be unknown because they did not show up in the training data.</p>
    <p>So I decided to tokenize the prompts at a character level which would allow for more flexibility in what the user types in. In hindsight, it probably still would have been fine to tokenize at the word level and use an "UNKNOWN" token for words not encountered before.</p>
    <p>Next, I chose to tokenize the replies at the word level. This allows the chatbot to reply with a fixed set of words that Maya actually used in the game. It also prevents the bot from coming up with random jumbles of letters.</p>
    <p>This is very interesting because in most sequence-to-sequence problems, the tokenzing is done at the same level for both input and output. I was curious to see what was going to happen if I did the tokenizing differently for the input and output sequences.
    </p>
    <p>To preprocess the prompts, I built a dictionary that maps each unique character to a unique index value and vice versa.
    <br>For the replies, I did the same thing except each unique WORD is mapped to a unique index value and vice versa.
    </p>
    <p>Here are the first 10 key-value pairs in the word dictionary:
        <pre style="font-size:16px">
    {0: 'PAD', 
    1: 'missile', 
    2: 'mmm', 
    3: 'unsolved', 
    4: 'overwhelmed', 
    5: 'untold', 
    6: 'classmate', 
    7: 'confess', 
    8: 'ooh', 
    9: 'nooooooooooo', 
    10: 'said'
    ...}
        </pre>
    </p>
    <p>For each prompt, I converted every character into its index value and then one-hot encoded those values. The result of each prompt is a matrix of shape:<br> (max line length, number of unique characters)</p>
    <p>For each reply, I converted each word to its index value and left it as a single array of values. This serves as the second input for the model.
    <br>Then I did the same thing but instead also one-hot encoded the values. This was the target output of the model.
    <p>You can see how the phrase below gets converted to an array of values based on the dictionary above.</p>
    <pre style="font-size:16px">
    "classmate said missle"
    
    [6, 10, 1]
    </pre>

    </p>
    <p>Once all the preprocessing was done, I ended up with a huge array of values which represented the text. 
    <br>Finally I could build the model and start training it. </p>
    <span style="font-size:28px; font-family:arial;"><u>The Model</u></span>
    <p>The model is largely identical to the one in the <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html">Keras Blog</a> post on seq2seq models except I am using an added embedding layer for the decoder input.
    <pre class="code"><code>
    hidden_dim = 64
    
    encoder_inputs = Input(shape=(None, prompt_feature_size))
    encoder = LSTM(hidden_dim, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)
    encoder_states = [state_h, state_c]

    decoder_inputs = Input(shape=(None,))
    decoder_embed = Embedding(reply_feature_size, hidden_dim)
    embedded = decoder_embed(decoder_inputs)

    decoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(embedded,
                                         initial_state=encoder_states)
    decoder_dense = Dense(reply_feature_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)
    
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    
    model.summary()      
    </code></pre>
    <pre style="font-size:12px">
_______________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
===============================================================================================
input_2 (InputLayer)            (None, None)         0                                            
_______________________________________________________________________________________________
input_1 (InputLayer)            (None, None, 52)     0                                            
_______________________________________________________________________________________________
embedding_1 (Embedding)         (None, None, 64)     113088      input_2[0][0]                    
_______________________________________________________________________________________________
lstm_1 (LSTM)                   [(None, 64), (None,  29952       input_1[0][0]                    
_______________________________________________________________________________________________
lstm_2 (LSTM)                   [(None, None, 64), ( 33024       embedding_1[0][0]                
                                                                 lstm_1[0][1]                     
                                                                 lstm_1[0][2]                     
_______________________________________________________________________________________________
dense_1 (Dense)                 (None, None, 1767)   114855      lstm_2[0][0]                     
===============================================================================================
Total params: 290,919
Trainable params: 290,919
Non-trainable params: 0
_______________________________________________________________________________________________
    </pre></p>
    <p>I trained the model for 1000 epochs which was just an arbitrary choice. Note that I didn't need to hold out any data for validation since there isn't really such a thing as a "correct" reply to a prompt. I just wanted any reply that hopefully makes some kind of sense.
        <pre class="code"><code>
    model.fit([X, y_input], y_output, epochs=1000)
        </code></pre>
    </p>
    <p>Training took about 2.5 hours on a CPU and the loss had yet to converge.
</p>
    <p>Before using the trained model, I had to write some helper functions to process any prompts a human could give it and also to change the model output back into text.
</p>
    <span style="font-size:28px; font-family:arial;"><u>Results</u></span>
    <p>Now we can start interacting with our chatbot.
    <pre style="font-size:16px">
    Me: hello there
    MayaBot: hey hey larry . whats he talking about ? 
    
    Me: who is larry?
    MayaBot: hey hey nick . 
    
    Me: i am not nick
    MayaBot: i was sure it was because the lake looked like a gourd when viewed from above ! 
    
    Me: lake?
    MayaBot: nick ?
    
    Me: i told you i am not nick
    MayaBot: and im she sure then morning maybe this doctor is a lie !
    
    Me: that doesn't make sense
    MayaBot: i wonder why the caretaker didnt take the letter with him ? 
    
    Me: ok bye
    MayaBot: nick ? 
    </pre>

    </p>
    
    <p>
        Right away we can see that a lot of the replies don't fit the context of the prompt. That is likely because most of the game dialogue is not really how normal everyday conversations would go. 
        <br>So the bot would not have learned how to respond properly to simple questions like "how are you doing?"</p>
    <p>It is interesting to see that the bot has captured some of Maya's personality. In the game she says "Nick" a lot, which is her nickname for the main protagonist. She also says "...!" or "What!?" a lot too which the bot seems to have picked up on.</p>
    <p>If you want to interact with the bot yourself, I made a fun demo <a href="https://gippoo.github.io/chatbot/demo/">here</a>.
    
    
</div>
<div style="height: 100px"></div>
</body>
</html>
