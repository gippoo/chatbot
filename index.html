<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Gary Ip - Chatbot</title>
<link rel="shortcut icon" href="https://github.com/gippoo/gippoo.github.io/raw/master/favicon.ico" type="image/x-icon">
<link rel="icon" href="https://github.com/gippoo/gippoo.github.io/raw/master/favicon.ico" type="image/x-icon">

<style>
    body {
        font-family: "Trebuchet MS";
    }
    
    a.header:link {
        color: #bfd8ff;
    }
    a.header:visited {
        color: #bfd8ff;
    }  
    a.header:hover {
        color: #6da6ff;
    }
    
    div.header {
        position: absolute;
        left: 0;
        top: 0;
        right: 0;
        height: 54px;
        background-color: #4a70ad;
    }
    
    div.title_text {
        position: absolute;
        left: 10px;
        top: 10px;
        font-size: 24px;
	    color: white;
    }
    
    pre.code {
        font-size: 14px;
        background-color: #dee1e3;
        width: 600px;
    }
</style>

</head>


<body>


<div class="header">
    <div class="title_text">
    	<a class="header" href="https://gippoo.github.io/" style="text-decoration:none;">Gary Ip</a> | 
        <a class="header" href="https://gippoo.github.io/resume/" style="text-decoration:none;">Resume</a> | 
        <a class="header" href="https://github.com/gippoo" style="text-decoration:none;">GitHub</a> | 
        <a class="header" href="https://www.linkedin.com/in/gary-ip27/" style="text-decoration:none;">LinkedIn</a>
    </div>
</div>


<div style="position: relative; top: 54px; text-align: center; font-family: Times">
    <figure>
        <img src="https://vignette.wikia.nocookie.net/aceattorney/images/0/02/Maya_Fey_Portrait_AA6.png/revision/latest/scale-to-width-down/310?cb=20160407182727" width="155" height="232"/>
        <figcaption style="font-size: 12px;">Source: https://aceattorney.fandom.com/wiki/Maya_Fey</figcaption>
    </figure>
    <span style="font-size:32px;">Building a Chatbot</span>
</div>
<div style="position: relative; top: 54px; margin:auto; width: 700px; font-size:20px; font-family: Times;">
    <p><i>
        If you just want to see a demo, click <a href="https://gippoo.github.io/chatbot/demo/">here</a>.
    </i></p>
    <p>After watching I'm not a Robot and obsessing over it, I was inspired to create some kind of AI you could interact with in a human-like way. Making a chatbot seemed like a good place to start.
        <br>Rather than making a rule-based chatbot with several hard coded responses, I will be training it using deep learning techniques. So the outcome of this chatbot will depend on the data I use. What data should I use?</p>
    <p>Since this chatbot is meant to have conversations with people, I needed a dataset containing pairs of prompts and replies. Through some Google searches, I discovered that most examples of conversational chatbots are trained on the <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie Corpus</a> or a dataset of <a href="https://www.kaggle.com/reddit/reddit-comments-may-2015">Reddit comments</a>.</p>
    <p>I didn't want to do something that was already done so to put my own spin on it. I decided to train my chatbot on dialogue of a character from a video game. Wouldn't it be so cool if you could talk to your favorite video game character and get a response from them? <br>I figured that the Phoenix Wright series would be great for this as the games are extremely text heavy and have many exchanges between characters. I settled on using Maya (pictured above) to be the character the chatbot would learn from.</p>
    </p>
    <p>Fortunately, the games have been transcribed by fans. I will be using a <a href="https://gamefaqs.gamespot.com/ds/925589-phoenix-wright-ace-attorney/faqs/42767">game script</a> from the first game in the series.
    <br>Here is a small snippet from the script:
        <pre style="font-size:16px">
            -Maya
            Ph-Phoenix...?
            
            -Phoenix
            Well... court will be opening
            for session soon.
            
            -Maya
            What? But wait!
            
            -Maya
            Your defense attorney isn't
            even here yet! He's not...
            
            -Phoenix
            I'll be defending myself.
            
            -Maya
            Whaaaat!?
            
            -Phoenix
            Okay, let's do this.
        </pre>
    </p>
    <p>Unfortunately, the entire document was not always this clean. There would be a lot of random extra spaces or symbols used. For example some parts of the script looked like this:
        <pre style="font-size:16px">
            * -Maya
            * Maybe it should just be
            * a big "L" for "Lawyer"?
            *
            * -Phoenix
            * Hmm... I'm not so
            * sure about that.
            *
            ********************************************
            
            ***Present other****************************
            *
            * -Phoenix
            * Um, Detective.
        </pre>
    </p>
    <p>I did some basic cleaning first using the built in 'Find and Replace' functionality of notepad and then using Python string manipulations, such as making all text lowercase.
        <br>Then I wrote an algorithm to extract all of Maya's lines, which are the 'replies', and also the previous line right before each of Maya's lines, which are the 'prompts'.
    </p>
    <p>In the end we are left with 2116 prompt-reply pairs. This is a rather small collection considering the amount of text that typically gets used in NLP projects. <br>Let's look at the first 5 of these prompt-reply pairs:

    </p>
    <p>
        <pre style="font-size:16px">
        PROMPT: it's okay. i work here.
        REPLY: maya...
        
        
        PROMPT: maya...
        REPLY: maya fey.
        
        
        PROMPT: ...
        REPLY: i came in... the room was dark.
        
        
        PROMPT: i came in... the room was dark.
        REPLY: and sis... sis...!
        
        
        PROMPT: so, you're the chief's...?
        REPLY: sister. i'm her younger sister.
        </pre>
    </p>
    
    <p>Some improvements could be made here. In the original script, Maya sometimes says multiple lines in succession so, when extracting prompt and reply pairs, there are cases where she sometimes responds to herself. You can see this in the output above where she replies to her own reply. <br>There are probably some regex expressions that could be used to address this but I couldn't think of anything simple to implement. So I decided to proceed with what I had which should still be okay since we mainly care about gathering data on Maya's replies to ANYTHING (even herself).</p>
    
    <p>Next, I chose to tokenize the text at a character level. In hindsight, the word level was probably a much better choice but I at the time, I just wanted to get something working and didn't want to deal with any more preprocessing. I will definitely come back and try this with word level tokenizing.</p>
    <p>
    The model I am using is the LSTM encoder-decoder model outlined in the <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html">Keras Blog</a> post on seq2seq models. 
    </p>
    <p>This requires us to have 3 arrays of data to use for training: the input prompt, the input reply, and the output reply. <br>-The input prompt is simply an array of the prompts. <br>-The input reply is each reply with a START token placed at the beginning. <br>-The output replies have no START token but instead have an END token placed at the end of each line. The result is two arrays of replies with the output replies offset by 1 position.</p>
    <p>To preprocess the text, we build a dictionary that maps each character to a unique index value. We actually need two of these dictionaries, one for the prompts, and one for the replies since there may be characters present in the prompts which are not in the replies and vice versa. While we're at it, we can also create dictionaries that map the index value back to the character.
        <pre class="code"><code>
    prompt_chars = set(' '.join(prompt))
    reply_chars = set(' '.join(reply))
    
    prompt_char_to_ix = dict((c, i) for i, c in enumerate(prompt_chars))
    prompt_ix_to_char = dict((i, c) for i, c in enumerate(prompt_chars))
    reply_char_to_ix = dict((c, i) for i, c in enumerate(reply_chars))
    reply_ix_to_char = dict((i, c) for i, c in enumerate(reply_chars))
        </code></pre>
    </p>
    <p>Now, for each line of dialouge, we convert each character into its index value and then one-hot encode each value. The result of each line is a matrix of shape:<br> (max line length, number of unique characters) 
    <pre class="code"><code>
    # Processing the prompts
    X = []
    
    for line in prompt:
        one_hot = np.zeros((max_prompt_len, prompt_feature_size))
        x_seq = [prompt_char_to_ix[c] for c in line]
        for j in range(len(x_seq)):
            one_hot[j][x_seq[j]] = 1
        X.append(one_hot)
    X = np.asarray(X)
    
    # Processing the replies
    # Function to add start and end tokens
    def start_end(line):
        line = '\t'+line+'\n'
        return line
        
    Y = [start_end(line) for line in reply]
    
    y = []

    for line in Y:
        one_hot = np.zeros((max_reply_len, reply_feature_size))
        y_seq = [reply_char_to_ix[c] for c in line]
        for j in range(len(y_seq)):
            one_hot[j][y_seq[j]] = 1
        y.append(one_hot)
    y = np.asarray(y)
    
    # Split replies into input and output replies
    y_input = y[:,:max_reply_len-1,:]
    y_output = y[:,1:,:]
    </code></pre>

    </p>

    <p>Finally we can build the model and start training. The model is largely identical to the one in the Keras Blog.
    <pre class="code"><code>
    encoder_inputs = Input(shape=(None, prompt_feature_size))
    encoder = LSTM(128, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)
    encoder_states = [state_h, state_c]

    decoder_inputs = Input(shape=(None, reply_feature_size))
    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,
                                         initial_state=encoder_states)
    decoder_dense = Dense(reply_feature_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)
    
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    
    model.summary()        
        
    </code></pre>
    <pre style="font-size:12px">
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, None, 52)     0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, None, 51)     0                                            
__________________________________________________________________________________________________
lstm_3 (LSTM)                   [(None, 128), (None, 92672       input_3[0][0]                    
__________________________________________________________________________________________________
lstm_4 (LSTM)                   [(None, None, 128),  92160       input_4[0][0]                    
                                                                 lstm_3[0][1]                     
                                                                 lstm_3[0][2]                     
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, None, 51)     6579        lstm_4[0][0]                     
==================================================================================================
Total params: 191,411
Trainable params: 191,411
Non-trainable params: 0
    </pre></p>
    <p>I trained the model for 700 epochs which was just an arbitrary choice. Note that I didn't need to hold out any data for validation since there isn't really such a thing as a "correct" reply to a prompt. We just want any reply that hopefully makes some kind of sense.
        <pre class="code"><code>
    model.fit([X, y_input], y_output, epochs=700)
        </code></pre>
    </p>
    <p>Before using the trained model, we need some helper functions to process any prompts we give it and also to change the model output back into text.
    <pre class="code"><code>
    # Function to process our own prompts
    def process_question(question):
        question_vec = [prompt_char_to_ix[c] for c in question]
        
        one_hot = np.zeros((max_prompt_len, prompt_feature_size))
        
        for j in range(len(question_vec)):
            one_hot[j][question_vec[j]] = 1
        
        return one_hot.reshape(-1,max_prompt_len,prompt_feature_size)
        
    # Function to get a reply from the model and convert it to text
    def gen_reply(question):
        question = process_question(question)
        
        states_value = encoder_model.predict(question)
        
        target_seq = np.zeros((1, 1, reply_feature_size))
        target_seq[0, 0, reply_char_to_ix['\t']] = 1.
        
        stop_condition = False
        decoded_sentence = ''
        
        while not stop_condition:
            output_tokens, h, c = decoder_model.predict([target_seq] 
                                                        + states_value)
            
            sampled_token_index = np.argmax(output_tokens[0, -1, :])
            sampled_char = reply_ix_to_char[sampled_token_index]
            decoded_sentence += sampled_char
            
            if (sampled_char == '\n' 
                or len(decoded_sentence) > max_reply_len):
                stop_condition = True
    
            target_seq = np.zeros((1, 1, reply_feature_size))
            target_seq[0, 0, sampled_token_index] = 1.
            
            states_value = [h, c]
            
        return decoded_sentence
    </code></pre></p>
    <p>FINALLY we can start interacting with our chatbot.
    <pre style="font-size:16px">
    Me: hi
    MayaBot: i was really sorry'se callma!
    
    Me: what are you sorry about?
    MayaBot: this is the ficoston! too...
    
    Me: what is the ficoston?
    MayaBot: this is all tied to the dl-6 incident...
    
    Me: what happened in the incident?
    MayaBot: old wird mos like the sild, i'm noting on locken "sus "lissel, ergs!
    
    Me: uh that didn't make sense
    MayaBot: h-huh!? o-w-ow...
    
    Me: that actually made sense
    MayaBot: nick!
    
    Me: im not nick
    MayaBot: what!?
    
    Me: ok bye.
    MayaBot: nick! look! a lott!
    </pre>

    </p>
    
    <p>
        Right away we can see that the bot sometimes replies with words that aren't actually words. This is one of the disadvantages of using a character level model. A word level model would not have this issue.
        <br>Also, a lot of the replies don't fit the context of the prompt. That is likely because most of the game dialogue is not really how normal everyday conversations would go so the bot would not have learned how to respond properly to simple questions like "how are you doing?"</p>
    <p>It is interesting to see that the bot has captured some of Maya's personality. In the game she says "Nick!" a lot, which is her nickname for the main protagonist. She also says "Huh?" or "What!?" a lot too which the bot seems to have picked up on.</p>
    <p>If you want to interact with the bot yourself, I made a fun demo <a href="https://gippoo.github.io/chatbot/demo/">here</a>.
    
    
</div>
<div style="height: 100px"></div>
</body>
</html>
