{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('chatbotmodels/phoenixaa.txt').read().lower()\n",
    "text = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "\n",
    "for line in text:\n",
    "    if line != '' and line[0] not in ['-','+','*']:\n",
    "            cleaned_text.append(line)\n",
    "\n",
    "ix = 0\n",
    "new_cleaned_text = []\n",
    "\n",
    "for line in cleaned_text:\n",
    "    if line[-1] == \":\":\n",
    "        new_cleaned_text.append(line)\n",
    "        fixed_line = []\n",
    "        for next_line in cleaned_text[ix+1:]:\n",
    "            if next_line[-1] == \":\":\n",
    "                break\n",
    "            else:\n",
    "                fixed_line.append(next_line)\n",
    "        new_cleaned_text.append(' '.join(fixed_line))\n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = new_cleaned_text\n",
    "\n",
    "def get_dialouge(character):\n",
    "    char_dialouge = []\n",
    "    prompt = []\n",
    "    ix = 0\n",
    "    for line in text:\n",
    "        if line == character+':':\n",
    "            char_dialouge.append(text[ix+1])\n",
    "            prompt.append(text[ix-1])\n",
    "        ix += 1\n",
    "    return prompt, char_dialouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, reply = get_dialouge('maya')\n",
    "aadf = pd.DataFrame({'prompt':prompt, 'reply':reply})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('phoenix.txt').read()\n",
    "test = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "cleaned_text = []\n",
    "ix = 0\n",
    "\n",
    "for i in test:\n",
    "    if i != '':\n",
    "        if i[0] == '-' and i[1] not in ['-','=']:\n",
    "            fix_ix = ix + 1\n",
    "            fixed_line = []\n",
    "            cleaned_text.append(i)\n",
    "            for line in test[ix+1:]:\n",
    "                if line in ['', ' ', '  ', '   ', '    ']:\n",
    "                    break\n",
    "                else:\n",
    "                    fixed_line.append(line)\n",
    "            fixed_line = ' '.join(' '.join(fixed_line).split())\n",
    "            cleaned_text.append(fixed_line)\n",
    "    ix += 1\n",
    "cleaned_text = '\\n'.join(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = cleaned_text.split('\\n')\n",
    "\n",
    "def get_dialouge(character):\n",
    "    char_dialouge = []\n",
    "    prompt = []\n",
    "    ix = 0\n",
    "    for line in list_text:\n",
    "        if line == '-'+character and list_text[ix+1] != '...':\n",
    "            char_dialouge.append(list_text[ix+1].lower())\n",
    "            prompt.append(list_text[ix-1].lower())\n",
    "        ix += 1\n",
    "    return prompt, char_dialouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, reply = get_dialouge('Maya')\n",
    "aaadf = pd.DataFrame({'prompt':prompt, 'reply':reply})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([aadf,aaadf], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = list(df['prompt'])\n",
    "reply = list(df['reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(i hope maya's ok. i should hurry and get to the detention center, asap!) june 20, 10:34 am detention center visitor's room\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"that... that's terrible... why? why would my aunt...? everything is going just as i predicted, mr. phoenix wright.\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply[332]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt max line length: 123\n",
      "143\n",
      "Reply max line length: 114\n",
      "332\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "max_position = 0\n",
    "count = 0\n",
    "for i in prompt:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        max_position = count\n",
    "    count += 1\n",
    "print(\"Prompt max line length: \"+str(max_len))\n",
    "print(max_position)\n",
    "\n",
    "max_position = 0\n",
    "count = 0\n",
    "max_len = 0\n",
    "for i in reply:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        max_position = count\n",
    "    count += 1\n",
    "print(\"Reply max line length: \"+str(max_len))\n",
    "print(max_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for line in reply:\n",
    "    y.append(line[:max_seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_end(line):\n",
    "    line = '\\t'+line+'\\n'\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [start_end(line) for line in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad(line):\n",
    "#     if len(line) < max_seq_len:\n",
    "#         line += ' '*(max_seq_len-len(line))\n",
    "#     return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_padded = [pad(line) for line in prompt]\n",
    "# y_padded = [pad(line) for line in reply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_chars = set(' '.join(prompt))\n",
    "reply_chars = set(' '.join(Y))\n",
    "# all_chars = []\n",
    "\n",
    "# for i in prompt_chars:\n",
    "#     if i not in all_chars:\n",
    "#         all_chars.append(i)\n",
    "# for i in reply_chars:\n",
    "#     if i not in all_chars:\n",
    "#         all_chars.append(i)\n",
    "# all_chars = sorted(list(set(all_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_char_to_ix = dict((c, i) for i, c in enumerate(prompt_chars))\n",
    "prompt_ix_to_char = dict((i, c) for i, c in enumerate(prompt_chars))\n",
    "reply_char_to_ix = dict((c, i) for i, c in enumerate(reply_chars))\n",
    "reply_ix_to_char = dict((i, c) for i, c in enumerate(reply_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_chars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-ecceeb85a6d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchar_to_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mix_to_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_chars' is not defined"
     ]
    }
   ],
   "source": [
    "# char_to_ix = dict((c, i) for i, c in enumerate(all_chars))\n",
    "# ix_to_char = dict((i, c) for i, c in enumerate(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_feature_size = len(prompt_chars)\n",
    "reply_feature_size = len(reply_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT DATA SHAPE: (2709, 40, 53)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "\n",
    "for i in range(len(prompt)):\n",
    "    x_seq = prompt[i][:max_seq_len]\n",
    "    one_hot = np.zeros((max_seq_len, prompt_feature_size))\n",
    "    x_seq = [prompt_char_to_ix[c] for c in x_seq]\n",
    "    for j in range(len(x_seq)):\n",
    "        one_hot[j][x_seq[j]] = 1\n",
    "    X.append(one_hot)\n",
    "X = np.asarray(X)\n",
    "\n",
    "print(\"INPUT DATA SHAPE: \"+str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT DATA SHAPE: (2709, 40, 54)\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    y_seq= Y[i][:max_seq_len]\n",
    "    one_hot = np.zeros((max_seq_len, reply_feature_size))\n",
    "    y_seq = [reply_char_to_ix[c] for c in y_seq]\n",
    "    for j in range(len(y_seq)):\n",
    "        one_hot[j][y_seq[j]] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(\"OUTPUT DATA SHAPE: \"+str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2709, 39, 54)\n",
      "(2709, 39, 54)\n"
     ]
    }
   ],
   "source": [
    "y_input = y[:,:39,:]\n",
    "print(y_input.shape)\n",
    "y_output = y[:,1:,:]\n",
    "print(y_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gary\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed, Activation, RepeatVector, Input\n",
    "from keras.callbacks import ModelCheckpoint, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 53)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 54)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 93184       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 128),  93696       input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 54)     6966        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 193,846\n",
      "Trainable params: 193,846\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, prompt_feature_size))\n",
    "encoder = LSTM(128, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, reply_feature_size))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(reply_feature_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1766A: 0s - loss: 0\n",
      "Epoch 2/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1733\n",
      "Epoch 3/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1745\n",
      "Epoch 4/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1732\n",
      "Epoch 5/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1754\n",
      "Epoch 6/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1693\n",
      "Epoch 7/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1718\n",
      "Epoch 8/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1727\n",
      "Epoch 9/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1717\n",
      "Epoch 10/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1685\n",
      "Epoch 11/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1681\n",
      "Epoch 12/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1722\n",
      "Epoch 13/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1696\n",
      "Epoch 14/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1703\n",
      "Epoch 15/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1696\n",
      "Epoch 16/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1664\n",
      "Epoch 17/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1677\n",
      "Epoch 18/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1648\n",
      "Epoch 19/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1679\n",
      "Epoch 20/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1693\n",
      "Epoch 21/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1644\n",
      "Epoch 22/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1647\n",
      "Epoch 23/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1650\n",
      "Epoch 24/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1624\n",
      "Epoch 25/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1641\n",
      "Epoch 26/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1619\n",
      "Epoch 27/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1641\n",
      "Epoch 28/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1626\n",
      "Epoch 29/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1606\n",
      "Epoch 30/30\n",
      "2709/2709 [==============================] - 5s 2ms/step - loss: 0.1610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296e0cc67f0>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, y_input], y_output,\n",
    "          epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(128,))\n",
    "decoder_state_input_c = Input(shape=(128,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question):\n",
    "    question_vec = [prompt_char_to_ix[c] for c in question]\n",
    "    \n",
    "    one_hot = np.zeros((max_seq_len, prompt_feature_size))\n",
    "    \n",
    "    for j in range(len(question_vec)):\n",
    "        one_hot[j][question_vec[j]] = 1\n",
    "    \n",
    "    return one_hot.reshape(-1,max_seq_len,prompt_feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_reply(question):\n",
    "    question = process_question(question)\n",
    "    \n",
    "    states_value = encoder_model.predict(question)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, reply_feature_size))\n",
    "    target_seq[0, 0, reply_char_to_ix['\\t']] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reply_ix_to_char[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, reply_feature_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i kind of wish she'd said a little more.\n",
      "\n",
      "\n",
      "it's hard. i think she's really will.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand = prompt[np.random.randint(len(prompt))][:max_seq_len]\n",
    "print(rand)\n",
    "print('\\n')\n",
    "print(gen_reply(rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey casey...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gen_reply(new_cleaned_text[np.random.randint(len(new_cleaned_text))][:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('chatbotweights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gary\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\topology.py:2364: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "C:\\Users\\Gary\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\topology.py:2364: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_6:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_7:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('chatbot.h5')\n",
    "encoder_model.save('chatbotenc.h5')\n",
    "decoder_model.save('chatbotdec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               92672     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 128)           131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 52)            6708      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 52)            0         \n",
      "=================================================================\n",
      "Total params: 230,964\n",
      "Trainable params: 230,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_seq_len, feature_size)))\n",
    "model.add(RepeatVector(max_seq_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(feature_size)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8247\n",
      "Epoch 2/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8229\n",
      "Epoch 3/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8253A: 1s - \n",
      "Epoch 4/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8228A: 0s - loss: 0.8\n",
      "Epoch 5/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8300\n",
      "Epoch 6/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8309\n",
      "Epoch 7/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8140\n",
      "Epoch 8/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8252\n",
      "Epoch 9/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8170\n",
      "Epoch 10/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8137\n",
      "Epoch 11/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8145A: 1s\n",
      "Epoch 12/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8160\n",
      "Epoch 13/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8153\n",
      "Epoch 14/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8123\n",
      "Epoch 15/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8117A: 0s - l\n",
      "Epoch 16/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8055\n",
      "Epoch 17/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8178\n",
      "Epoch 18/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8154\n",
      "Epoch 19/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8025\n",
      "Epoch 20/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7985\n",
      "Epoch 21/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8127\n",
      "Epoch 22/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8052\n",
      "Epoch 23/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8080\n",
      "Epoch 24/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.8013\n",
      "Epoch 25/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7950\n",
      "Epoch 26/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7924\n",
      "Epoch 27/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7945\n",
      "Epoch 28/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7931\n",
      "Epoch 29/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7904\n",
      "Epoch 30/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7883\n",
      "Epoch 31/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7927\n",
      "Epoch 32/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7972\n",
      "Epoch 33/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7850\n",
      "Epoch 34/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7757A: 0s - l\n",
      "Epoch 35/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7771\n",
      "Epoch 36/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7792\n",
      "Epoch 37/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7777\n",
      "Epoch 38/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7985\n",
      "Epoch 39/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7735\n",
      "Epoch 40/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7781\n",
      "Epoch 41/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7739\n",
      "Epoch 42/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7776A: 0s - loss: 0.775\n",
      "Epoch 43/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7706\n",
      "Epoch 44/500\n",
      "2116/2116 [==============================] - 3s 2ms/step - loss: 0.7685\n",
      "Epoch 45/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7748A: 0s - loss\n",
      "Epoch 46/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7710\n",
      "Epoch 47/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7613\n",
      "Epoch 48/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7601\n",
      "Epoch 49/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7649\n",
      "Epoch 50/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7697\n",
      "Epoch 51/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7651\n",
      "Epoch 52/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7637\n",
      "Epoch 53/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7659\n",
      "Epoch 54/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7726\n",
      "Epoch 55/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7544\n",
      "Epoch 56/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7571\n",
      "Epoch 57/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7563\n",
      "Epoch 58/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7550\n",
      "Epoch 59/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7664\n",
      "Epoch 60/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7504A: 2s - - ETA: 1s - loss: 0 - ETA: 0s - loss:\n",
      "Epoch 61/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7474\n",
      "Epoch 62/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7572\n",
      "Epoch 63/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7407\n",
      "Epoch 64/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7425\n",
      "Epoch 65/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7400\n",
      "Epoch 66/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7482\n",
      "Epoch 67/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7502\n",
      "Epoch 68/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7429\n",
      "Epoch 69/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7435\n",
      "Epoch 70/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7508\n",
      "Epoch 71/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7376A: 0s - loss: 0.7\n",
      "Epoch 72/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7328\n",
      "Epoch 73/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7485\n",
      "Epoch 74/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7423\n",
      "Epoch 75/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7333\n",
      "Epoch 76/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7310\n",
      "Epoch 77/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7342\n",
      "Epoch 78/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7320\n",
      "Epoch 79/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7243\n",
      "Epoch 80/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7237\n",
      "Epoch 81/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7236\n",
      "Epoch 82/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7351\n",
      "Epoch 83/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7195\n",
      "Epoch 84/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7259A: 0s - l\n",
      "Epoch 85/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7191\n",
      "Epoch 86/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7234\n",
      "Epoch 87/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7219\n",
      "Epoch 88/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7176\n",
      "Epoch 89/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7123\n",
      "Epoch 90/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7270\n",
      "Epoch 91/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7054\n",
      "Epoch 92/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7198\n",
      "Epoch 93/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7172\n",
      "Epoch 94/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7096\n",
      "Epoch 95/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7282\n",
      "Epoch 96/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7101\n",
      "Epoch 97/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7088\n",
      "Epoch 98/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7080\n",
      "Epoch 99/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7097\n",
      "Epoch 100/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7145\n",
      "Epoch 101/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6956\n",
      "Epoch 102/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7026\n",
      "Epoch 103/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7076\n",
      "Epoch 104/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7002\n",
      "Epoch 105/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7046\n",
      "Epoch 106/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6984\n",
      "Epoch 107/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6955\n",
      "Epoch 108/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.7067\n",
      "Epoch 109/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6922\n",
      "Epoch 110/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6889\n",
      "Epoch 111/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6936\n",
      "Epoch 112/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6952A: 1s\n",
      "Epoch 113/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6933\n",
      "Epoch 114/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6862\n",
      "Epoch 115/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6950\n",
      "Epoch 116/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6886\n",
      "Epoch 117/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6950\n",
      "Epoch 118/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6902\n",
      "Epoch 119/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6794\n",
      "Epoch 120/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6880\n",
      "Epoch 121/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6987\n",
      "Epoch 122/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6778\n",
      "Epoch 123/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6752\n",
      "Epoch 124/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6882\n",
      "Epoch 125/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6952\n",
      "Epoch 126/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6694\n",
      "Epoch 127/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6865A: 0s - l\n",
      "Epoch 128/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6601\n",
      "Epoch 129/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6748\n",
      "Epoch 130/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6876\n",
      "Epoch 131/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6748\n",
      "Epoch 132/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6827\n",
      "Epoch 133/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6676\n",
      "Epoch 134/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6697\n",
      "Epoch 135/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6821\n",
      "Epoch 136/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6726A: 0s - lo\n",
      "Epoch 137/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6723\n",
      "Epoch 138/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6893\n",
      "Epoch 139/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6732\n",
      "Epoch 140/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6770\n",
      "Epoch 141/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6747\n",
      "Epoch 142/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6702\n",
      "Epoch 143/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6578\n",
      "Epoch 144/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6630\n",
      "Epoch 145/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6729\n",
      "Epoch 146/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6433\n",
      "Epoch 147/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6692\n",
      "Epoch 148/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6647\n",
      "Epoch 149/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6668\n",
      "Epoch 150/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6624A: 1s\n",
      "Epoch 151/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6628\n",
      "Epoch 152/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6468\n",
      "Epoch 153/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6536\n",
      "Epoch 154/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6526\n",
      "Epoch 155/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6527\n",
      "Epoch 156/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6525\n",
      "Epoch 157/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6509\n",
      "Epoch 158/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6757A: 0s - loss: 0.66\n",
      "Epoch 159/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6547\n",
      "Epoch 160/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6652\n",
      "Epoch 161/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6501\n",
      "Epoch 162/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6485\n",
      "Epoch 163/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6517\n",
      "Epoch 164/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6424\n",
      "Epoch 165/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6517\n",
      "Epoch 166/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6428A: 1s - lo - ETA: 0s - loss: 0\n",
      "Epoch 167/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6486\n",
      "Epoch 168/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6455\n",
      "Epoch 169/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6549\n",
      "Epoch 170/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6377\n",
      "Epoch 171/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6462\n",
      "Epoch 172/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6441\n",
      "Epoch 173/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6370\n",
      "Epoch 174/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6552\n",
      "Epoch 175/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6332A: 1s -\n",
      "Epoch 176/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6421\n",
      "Epoch 177/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6306\n",
      "Epoch 178/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6357\n",
      "Epoch 179/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6408\n",
      "Epoch 180/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6264\n",
      "Epoch 181/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6436\n",
      "Epoch 182/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6364\n",
      "Epoch 183/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6236\n",
      "Epoch 184/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6392\n",
      "Epoch 185/500\n",
      "2116/2116 [==============================] - 3s 2ms/step - loss: 0.6294\n",
      "Epoch 186/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6322\n",
      "Epoch 187/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6281\n",
      "Epoch 188/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6335\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6279\n",
      "Epoch 190/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6291\n",
      "Epoch 191/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6273\n",
      "Epoch 192/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6210\n",
      "Epoch 193/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6272\n",
      "Epoch 194/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6230\n",
      "Epoch 195/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6148\n",
      "Epoch 196/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6235\n",
      "Epoch 197/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6200\n",
      "Epoch 198/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6188\n",
      "Epoch 199/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6088\n",
      "Epoch 200/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6225\n",
      "Epoch 201/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6120\n",
      "Epoch 202/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6226\n",
      "Epoch 203/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6180\n",
      "Epoch 204/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6344\n",
      "Epoch 205/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6119A: 0s - loss: 0\n",
      "Epoch 206/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6111\n",
      "Epoch 207/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6190A: 0s - los\n",
      "Epoch 208/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6119\n",
      "Epoch 209/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6140\n",
      "Epoch 210/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6275A: 0s - loss\n",
      "Epoch 211/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6006\n",
      "Epoch 212/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6117A: 0s - lo\n",
      "Epoch 213/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6100\n",
      "Epoch 214/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6006\n",
      "Epoch 215/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6031A: 1s - \n",
      "Epoch 216/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6025A: \n",
      "Epoch 217/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6045A: 0s - loss: 0.6\n",
      "Epoch 218/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6169\n",
      "Epoch 219/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6023\n",
      "Epoch 220/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5969A: 1s - \n",
      "Epoch 221/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6069\n",
      "Epoch 222/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6196\n",
      "Epoch 223/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5994\n",
      "Epoch 224/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6070\n",
      "Epoch 225/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5987\n",
      "Epoch 226/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5947A: \n",
      "Epoch 227/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6067\n",
      "Epoch 228/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6070\n",
      "Epoch 229/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6146\n",
      "Epoch 230/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5875A: 0s - loss: 0.60 - ETA: 0s - loss\n",
      "Epoch 231/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5899\n",
      "Epoch 232/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5961\n",
      "Epoch 233/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6000\n",
      "Epoch 234/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6055\n",
      "Epoch 235/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6037\n",
      "Epoch 236/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6017\n",
      "Epoch 237/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.6017\n",
      "Epoch 238/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5910\n",
      "Epoch 239/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5862\n",
      "Epoch 240/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5982\n",
      "Epoch 241/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5937\n",
      "Epoch 242/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5937A:\n",
      "Epoch 243/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5999A: 1s \n",
      "Epoch 244/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5917\n",
      "Epoch 245/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5970A: 0s - loss: 0.\n",
      "Epoch 246/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5919A\n",
      "Epoch 247/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5939\n",
      "Epoch 248/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5798\n",
      "Epoch 249/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5899\n",
      "Epoch 250/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5773\n",
      "Epoch 251/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5959\n",
      "Epoch 252/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5778\n",
      "Epoch 253/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5801\n",
      "Epoch 254/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5845\n",
      "Epoch 255/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5888\n",
      "Epoch 256/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5783\n",
      "Epoch 257/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5892\n",
      "Epoch 258/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5751\n",
      "Epoch 259/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5819\n",
      "Epoch 260/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5777\n",
      "Epoch 261/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5944\n",
      "Epoch 262/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5730\n",
      "Epoch 263/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5801\n",
      "Epoch 264/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5824A: 0s - loss\n",
      "Epoch 265/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5712\n",
      "Epoch 266/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5706A: 0s - los\n",
      "Epoch 267/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5638\n",
      "Epoch 268/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5769\n",
      "Epoch 269/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5721\n",
      "Epoch 270/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5905\n",
      "Epoch 271/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5715A: 1s - los - ETA: 0s - los\n",
      "Epoch 272/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5824\n",
      "Epoch 273/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5816\n",
      "Epoch 274/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5764\n",
      "Epoch 275/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5732A: 0s - loss\n",
      "Epoch 276/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5672\n",
      "Epoch 277/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5929\n",
      "Epoch 278/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5775\n",
      "Epoch 279/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5790\n",
      "Epoch 280/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5723\n",
      "Epoch 281/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5788\n",
      "Epoch 283/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5786\n",
      "Epoch 284/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5721\n",
      "Epoch 285/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5594\n",
      "Epoch 286/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5638\n",
      "Epoch 287/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5733\n",
      "Epoch 288/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5570\n",
      "Epoch 289/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5689\n",
      "Epoch 290/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5702\n",
      "Epoch 291/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5525\n",
      "Epoch 292/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5678A: 1s - \n",
      "Epoch 293/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5706\n",
      "Epoch 294/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5725\n",
      "Epoch 295/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5710\n",
      "Epoch 296/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5606\n",
      "Epoch 297/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5603\n",
      "Epoch 298/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5558\n",
      "Epoch 299/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5558\n",
      "Epoch 300/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5748\n",
      "Epoch 301/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5521\n",
      "Epoch 302/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5607\n",
      "Epoch 303/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5572\n",
      "Epoch 304/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5523\n",
      "Epoch 305/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5653\n",
      "Epoch 306/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5588\n",
      "Epoch 307/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5549\n",
      "Epoch 308/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5589\n",
      "Epoch 309/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5489\n",
      "Epoch 310/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5639\n",
      "Epoch 311/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5646\n",
      "Epoch 312/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5482\n",
      "Epoch 313/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5681\n",
      "Epoch 314/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5590\n",
      "Epoch 315/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5519\n",
      "Epoch 316/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5560\n",
      "Epoch 317/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5621\n",
      "Epoch 318/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5544\n",
      "Epoch 319/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5516\n",
      "Epoch 320/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5431\n",
      "Epoch 321/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5437\n",
      "Epoch 322/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5555\n",
      "Epoch 323/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5557\n",
      "Epoch 324/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5547\n",
      "Epoch 325/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5528\n",
      "Epoch 326/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5458A: 0s - loss: 0.545\n",
      "Epoch 327/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5526\n",
      "Epoch 328/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5393\n",
      "Epoch 329/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5425\n",
      "Epoch 330/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5447A: 0s - los\n",
      "Epoch 331/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5434\n",
      "Epoch 332/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5405\n",
      "Epoch 333/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5375\n",
      "Epoch 334/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5425\n",
      "Epoch 335/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5286A: 0s - lo\n",
      "Epoch 336/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5444\n",
      "Epoch 337/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5434\n",
      "Epoch 338/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5286\n",
      "Epoch 339/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5297\n",
      "Epoch 340/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5328\n",
      "Epoch 341/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5351\n",
      "Epoch 342/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5408\n",
      "Epoch 343/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5266\n",
      "Epoch 344/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5376\n",
      "Epoch 345/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5568A: 0s - los\n",
      "Epoch 346/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5313\n",
      "Epoch 347/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5307A: 0s - loss: 0.5\n",
      "Epoch 348/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5451\n",
      "Epoch 349/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5225A: 0s - loss: 0\n",
      "Epoch 350/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5496\n",
      "Epoch 351/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5448\n",
      "Epoch 352/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5454\n",
      "Epoch 353/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5302\n",
      "Epoch 354/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5399\n",
      "Epoch 355/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5301\n",
      "Epoch 356/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5339\n",
      "Epoch 357/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5346\n",
      "Epoch 358/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5356\n",
      "Epoch 359/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5391\n",
      "Epoch 360/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5312\n",
      "Epoch 361/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5316A: 0s - loss: 0.5\n",
      "Epoch 362/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5292\n",
      "Epoch 363/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5260\n",
      "Epoch 364/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5386\n",
      "Epoch 365/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5214\n",
      "Epoch 366/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5296\n",
      "Epoch 367/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5236A: 0s - loss:\n",
      "Epoch 368/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5255\n",
      "Epoch 369/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5232\n",
      "Epoch 370/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5227\n",
      "Epoch 371/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5267\n",
      "Epoch 372/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5156\n",
      "Epoch 373/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5227\n",
      "Epoch 374/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5226\n",
      "Epoch 375/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5198\n",
      "Epoch 376/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5134A: 1s\n",
      "Epoch 377/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5073\n",
      "Epoch 378/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5153A: 1s \n",
      "Epoch 379/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5172A: 0s - los\n",
      "Epoch 380/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5194\n",
      "Epoch 381/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5196\n",
      "Epoch 382/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5161\n",
      "Epoch 383/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5280\n",
      "Epoch 384/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5218\n",
      "Epoch 385/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5202\n",
      "Epoch 386/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5199\n",
      "Epoch 387/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5173\n",
      "Epoch 388/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5248\n",
      "Epoch 389/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5286\n",
      "Epoch 390/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5284A: 0s - loss: 0.53\n",
      "Epoch 391/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5255\n",
      "Epoch 392/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5337\n",
      "Epoch 393/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5157\n",
      "Epoch 394/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5206\n",
      "Epoch 395/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5174\n",
      "Epoch 396/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5217\n",
      "Epoch 397/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5302\n",
      "Epoch 398/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5270\n",
      "Epoch 399/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5157\n",
      "Epoch 400/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5167\n",
      "Epoch 401/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5229\n",
      "Epoch 402/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5113\n",
      "Epoch 403/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5158\n",
      "Epoch 404/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5111\n",
      "Epoch 405/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5140\n",
      "Epoch 406/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5122\n",
      "Epoch 407/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5087\n",
      "Epoch 408/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5155\n",
      "Epoch 409/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5078\n",
      "Epoch 410/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5136\n",
      "Epoch 411/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5193\n",
      "Epoch 412/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5080\n",
      "Epoch 413/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5146\n",
      "Epoch 414/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5093\n",
      "Epoch 415/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5145\n",
      "Epoch 416/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5047\n",
      "Epoch 417/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5091\n",
      "Epoch 418/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5027\n",
      "Epoch 419/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5011\n",
      "Epoch 420/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5000\n",
      "Epoch 421/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5125\n",
      "Epoch 422/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5032A: \n",
      "Epoch 423/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4979\n",
      "Epoch 424/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5056\n",
      "Epoch 425/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5114\n",
      "Epoch 426/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5157\n",
      "Epoch 427/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5045\n",
      "Epoch 428/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5034\n",
      "Epoch 429/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5121\n",
      "Epoch 430/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5096\n",
      "Epoch 431/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5009A: 0s - loss:\n",
      "Epoch 432/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4977\n",
      "Epoch 433/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5039\n",
      "Epoch 434/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5060\n",
      "Epoch 435/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5047\n",
      "Epoch 436/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5013A: 0s - loss: 0.508 - ETA: 0s - loss: 0.5\n",
      "Epoch 437/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5003\n",
      "Epoch 438/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5004\n",
      "Epoch 439/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4940\n",
      "Epoch 440/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4943\n",
      "Epoch 441/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4994A: 0s - loss:\n",
      "Epoch 442/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4852\n",
      "Epoch 443/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4915\n",
      "Epoch 444/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4939\n",
      "Epoch 445/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5039\n",
      "Epoch 446/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5093\n",
      "Epoch 447/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5082\n",
      "Epoch 448/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5037\n",
      "Epoch 449/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4962\n",
      "Epoch 450/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4821A: 0s - los\n",
      "Epoch 451/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5029\n",
      "Epoch 452/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4950\n",
      "Epoch 453/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4917\n",
      "Epoch 454/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4916\n",
      "Epoch 455/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4848\n",
      "Epoch 456/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4978\n",
      "Epoch 457/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5013\n",
      "Epoch 458/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4864\n",
      "Epoch 459/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4875\n",
      "Epoch 460/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4896\n",
      "Epoch 461/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4945\n",
      "Epoch 462/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4963\n",
      "Epoch 463/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4924\n",
      "Epoch 464/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4797\n",
      "Epoch 465/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4845\n",
      "Epoch 466/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4863\n",
      "Epoch 467/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4961\n",
      "Epoch 468/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4917\n",
      "Epoch 469/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4892\n",
      "Epoch 470/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4812\n",
      "Epoch 471/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4881\n",
      "Epoch 472/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4875\n",
      "Epoch 473/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4808\n",
      "Epoch 474/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4916\n",
      "Epoch 475/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4933\n",
      "Epoch 476/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4822A: 0s - lo\n",
      "Epoch 477/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4879\n",
      "Epoch 478/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4997\n",
      "Epoch 479/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5033\n",
      "Epoch 480/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4847\n",
      "Epoch 481/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4828\n",
      "Epoch 482/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4852\n",
      "Epoch 483/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4895\n",
      "Epoch 484/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4795\n",
      "Epoch 485/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4952\n",
      "Epoch 486/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.5027\n",
      "Epoch 487/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4868\n",
      "Epoch 488/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4714\n",
      "Epoch 489/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4802\n",
      "Epoch 490/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4862\n",
      "Epoch 491/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4892\n",
      "Epoch 492/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4788A: 1s\n",
      "Epoch 493/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4949\n",
      "Epoch 494/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4904\n",
      "Epoch 495/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4793\n",
      "Epoch 496/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4849\n",
      "Epoch 497/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4760A: 2s - ETA: 0s - lo\n",
      "Epoch 498/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4829\n",
      "Epoch 499/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4848\n",
      "Epoch 500/500\n",
      "2116/2116 [==============================] - 3s 1ms/step - loss: 0.4840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2623200f9b0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question):\n",
    "    question = pad(question)\n",
    "    question_vec = [char_to_ix[c] for c in question]\n",
    "    \n",
    "    one_hot = np.zeros((max_seq_len, feature_size))\n",
    "    \n",
    "    for j in range(len(question_vec)):\n",
    "        one_hot[j][question_vec[j]] = 1\n",
    "    \n",
    "    return one_hot.reshape(-1,max_seq_len,feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(this girl is more troubled th\n",
      "\n",
      "\n",
      "look, a ladder!               "
     ]
    }
   ],
   "source": [
    "rand = prompt[np.random.randint(len(prompt))][:max_seq_len]\n",
    "print(rand)\n",
    "print('\\n')\n",
    "for i in np.argmax(model.predict(process_question(rand))[0],1):\n",
    "    print(ix_to_char[i], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yoof!iill!yllellyy aya.       "
     ]
    }
   ],
   "source": [
    "for i in np.argmax(model.predict(process_question('what do you want to eat?'))[0],1):\n",
    "    print(ix_to_char[i], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_char[np.argmax(model.predict(process_question(\"huh?\"))[0][3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
